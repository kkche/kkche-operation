
#build spark 1.0 by maven
#install scala 2.10.4 first

>tar xvzf spark-1.0.0.tgz
>sh make-distribution.sh --hadoop 2.4.0 --with-yarn --with-hive
# mvn clean package -Phadoop-2.4 -Phive -Pyarn -Dyarn.version=2.4.0 -Dhadoop.version=2.4.0 -DskipTests
>cp -r dist $HOME/software/spark-1.0.0-hadoop-2.4.0

#config spark
>cd ~/software/spark-1.0.0-hadoop-2.4.0/conf
>cp log4j.properties.template log4j.properties
>cp spark-env.sh.template spark-env.sh
>vim spark-env.sh
HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

>cp spark-defaults.conf.template spark-defaults.conf
>vim spark-defaults.conf
spark.master            yarn-cluster
spark.eventLog.dir      hdfs://<namenode>:8020/<directory>

>vim $HOME/.bash_profile
export SPARK_HOME=$HOME/software/spark-1.0.0-hadoop-2.4.0
export PATH=$SPARK_HOME/bin:$PATH
#export SPARK_JAR=$HOME/software/spark-1.0.0-hadoop-2.4.0/lib/spark-assembly-1.0.0-hadoop2.4.0.jar
export SPARK_JAR=hdfs://<namenode>:8020/user/$USER/spark/lib/spark-assembly-1.0.0-hadoop2.4.0.jar
>source $HOME/.bash_profile

>$HADOOP_HOME/bin/hadoop fs -ls -R /
>$HADOOP_HOME/bin/hadoop fs -mkdir -p /user/$USER/spark
>$HADOOP_HOME/bin/hadoop fs -put $SPARK_HOME/lib/spark-assembly-1.0.0-hadoop2.4.0.jar /user/$USER/spark/lib

#spark on yarn
>spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn-cluster \
    --num-executors 2 \
    --driver-memory 4g \
    --executor-memory 2g \
    --executor-cores 2  \
    $HOME/software/spark-1.0.0-hadoop-2.4.0/lib/spark-examples-1.0.0-hadoop2.4.0.jar \
    --arg 10

>spark-shell
http://<hostname>:4040
